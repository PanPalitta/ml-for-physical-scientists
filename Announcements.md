# Announcement
* [02-01-2021] In video 5, we've touched upon Perceptron model for binary classification, without showing the proof of convergence to the empirically optimal hypothesis that can successfully separate two linearly separable data. Since Perceptron Learning Algorithm (PLA) is the first algorithm known to men that automatically performs binary classification of linearly separable data, it might be worth taking a peek at the proof. This is also the first example of learning algorithm that relies on iteration to find the optimal parameter (remember that in linear regularization, we get the optimal parameter purely from linear algebra, without needing to iterates.) Typically, proof of convergence of these iterated dynamics is not tricky, but PLA has its simplicity yet non-trivial mathematically. So if you have time, please see [(this quite nicely written article)](https://towardsdatascience.com/perceptron-learning-algorithm-d5db0deab975) that sketches the proof of PLA convergence to an empirically optimal hypothesis, where the two classes of data are all correctly labeled. 

* [01-28-2021] Video lecture 3 and 4, as well as accompanying slides, are uploaded. 

* [01-25-2021] Homework 1 is posted [due Wednesday February 3 at Noon]! The total score is 60 points + 20 extra credits (challenging problems). In addition, those who wrote down the best solution to each problem or exercise will receive an extra 5 points! After watching video lecture 3 on regularized linear model you should be able to do homework 1.  Late submission will receive 40% penalty. For theoretical problems/exercises, feel free to write down the solution and attach the pdf files of your write-up and submit together with your ipynb file in a compressed zip file. 

* [01-21-2021] Video lecture 2 is shared [here](https://drive.google.com/drive/folders/1urRjPvKjLZU3QgEDolsQIoC2gssWHB3j?usp=sharing), and the workshop lecture is kindly prepared by Krittin [here](https://drive.google.com/drive/folders/1D72xllKe4zZxsA72R7srdMr6NTIt2xgx?usp=sharing). Please finish all these videos and feel free to post your questions to discuss on Monday on facebook page! 

* [01-19-2021] Video lecture 0 and 1 are shared on [Google Drive](https://drive.google.com/drive/folders/1urRjPvKjLZU3QgEDolsQIoC2gssWHB3j?usp=sharing). The accompanying slides are uploaded on GitHub [here](https://github.com/TChotibut/ml-for-physical-scientists/tree/main/Lecture%20Notes). Statistical Learning Theory is deep and sophisticated, please spend time thinking about concepts in lecture 1. A mathematically oriented student might also appreciate the formulation of supervised learning in the classic paper [F. Cucker and S. Smale, On the mathematical foundations of learning, Bulletin of the American Mathematical Society, 2002.](https://github.com/TChotibut/ml-for-physical-scientists/blob/main/Reading%20Materials/Week1_Cucker_Smale_Mathematical%20Foundations%20of%20Learning.pdf)

* [01-17-2021] Readings for week 1-2: Mehta's section 1-8.  **Preliminary homework 0** [here](https://github.com/sinonkt/ml-for-physical-scientists/blob/main/Homework/HW0_ML%20can%20be%20difficult.ipynb) (due Friday January 22) requires you to install Jupyter notebook and relevant packages such as sklearn on your local computer.  You'll get your hands dirty and get intuition why Machine Learning can be difficult. 

* [01-14-2021] Unless announced here or on the [facebook group](https://www.facebook.com/groups/1033694817095022), we will virtually meet every Monday from 11.00am-noon on [Zoom](https://chula.zoom.us/j/5943943895?pwd=dmpxc3NBMXFPam1FeGtTY2tsdm95UT09) ( Password: MLPhys ). In addition to the weekly virtual meeting, attendees are expected to learn privately at their convenience from the videos to be posted in the announcement ( 2-hour weekly lectures ). The purpose of the virtual meeting is to stimulate the discussion among attendees, as well as to have an interactive session in which questions or comments about lectures and problem sets shall be discussed.
